{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlflowlite Demo\n",
    "\n",
    "Four features. Zero config.\n",
    "\n",
    "1. **Automatic Tracing** - Every LLM call logged to MLflow\n",
    "2. **Prompt Versioning** - Git-like version control for prompts\n",
    "3. **AI Optimization** - Get specific improvement suggestions\n",
    "4. **Reliability** - Retry, timeout, and fallback support\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [The Scenario](#the-scenario)\n",
    "3. [Feature 1: Automatic Tracing](#feature-1-automatic-tracing)\n",
    "4. [Feature 2: Prompt Management & Versioning](#feature-2-prompt-management--versioning)\n",
    "5. [Feature 3: DSPy-Style Optimization](#feature-3-dspy-style-optimization)\n",
    "6. [Feature 4: Reliability Features](#feature-4-reliability-features)\n",
    "7. [What You Just Learned](#what-you-just-learned)\n",
    "8. [Advanced: Smart Routing & A/B Testing](#advanced-smart-routing--ab-testing)\n",
    "9. [Next Steps](#next-steps)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n",
      "🔑 API key configured\n",
      "📦 Ready to demonstrate:\n",
      "   1️⃣  Automatic MLflow Tracing\n",
      "   2️⃣  Prompt Management & Versioning\n",
      "   3️⃣  DSPy-Style Optimization\n",
      "   4️⃣  Reliability Features\n"
     ]
    }
   ],
   "source": [
    "# Install if needed (uncomment if running for first time)\n",
    "# !pip install -e .\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ⚠️ Set your API key here (or use .env file)\n",
    "# Option 1: Set directly (for quick demo)\n",
    "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = 'your-api-key-here'  # 👈 Replace with your key\n",
    "\n",
    "# Option 2: Load from .env file (recommended)\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# Force reload module (fixes Cursor/VS Code notebook caching)\n",
    "import sys\n",
    "if 'mlflowlite' in sys.modules:\n",
    "    del sys.modules['mlflowlite']\n",
    "\n",
    "# Import LiteLLM-style API\n",
    "import mlflowlite as mla\n",
    "from mlflowlite import Agent\n",
    "\n",
    "print(\"✅ Setup complete!\")\n",
    "if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'your-api-key-here':\n",
    "    print(\"🔑 API key configured\")\n",
    "else:\n",
    "    print(\"⚠️  Please set your ANTHROPIC_API_KEY in the cell above\")\n",
    "print(\"📦 Ready to demonstrate:\")\n",
    "print(\"   1️⃣  Automatic MLflow Tracing\")\n",
    "print(\"   2️⃣  Prompt Management & Versioning\")\n",
    "print(\"   3️⃣  DSPy-Style Optimization\")\n",
    "print(\"   4️⃣  Reliability Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📧 The Scenario: A Support Ticket\n",
    "\n",
    "Imagine you're building a support bot. You get this ticket:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Sample Support Ticket:\n",
      "\n",
      "Subject: Unable to access dashboard\n",
      "\n",
      "User reported that they cannot access the analytics dashboard.\n",
      "They receive a 403 Forbidden error when clicking on the dashboard link.\n",
      "User role: Manager\n",
      "Last successful access: 2 days ago\n",
      "Browser: Chrome 120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support_ticket = \"\"\"\n",
    "Subject: Unable to access dashboard\n",
    "\n",
    "User reported that they cannot access the analytics dashboard.\n",
    "They receive a 403 Forbidden error when clicking on the dashboard link.\n",
    "User role: Manager\n",
    "Last successful access: 2 days ago\n",
    "Browser: Chrome 120\n",
    "\"\"\"\n",
    "\n",
    "print(\"📋 Sample Support Ticket:\")\n",
    "print(support_ticket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📊 Feature 1: Automatic Tracing\n",
    "\n",
    "## The Old Way (Without Tracing)\n",
    "\n",
    "You call an LLM:\n",
    "```python\n",
    "response = openai.chat.completions.create(...)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Questions you can't answer:**\n",
    "- ❓ How much did that cost?\n",
    "- ❓ How long did it take?\n",
    "- ❓ Was the response quality good?\n",
    "- ❓ Can I compare this to yesterday's version?\n",
    "\n",
    "**You're flying blind! 🛩️💨**\n",
    "\n",
    "---\n",
    "\n",
    "## The New Way (With mlflowlite)\n",
    "\n",
    "**Same code, automatic insights:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Response:\n",
      "A manager reported being unable to access the analytics dashboard, receiving a 403 Forbidden error when clicking the link. The issue started 2 days ago, and the user is accessing the dashboard using Chrome version 120.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Make a simple call - automatically traced!\n",
    "response1 = mla.query(\n",
    "    model='claude-3-5-sonnet',\n",
    "    prompt='Summarize this support ticket in 2 sentences',\n",
    "    input=support_ticket\n",
    ")\n",
    "\n",
    "print(\"✅ Response:\")\n",
    "print(response1.content)\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Value Unlocked: See Everything Automatically\n",
    "\n",
    "**Look what you get for FREE:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📊 EVERYTHING TRACKED AUTOMATICALLY (Zero Config!)\n",
      "======================================================================\n",
      "\n",
      "💰 COST TRACKING:\n",
      "   Cost: $0.0010\n",
      "   Tokens: 126\n",
      "   👉 You'll see this coming BEFORE the bill arrives!\n",
      "\n",
      "⚡ PERFORMANCE:\n",
      "   Latency: 2.87s\n",
      "   👉 Catch slow responses early!\n",
      "\n",
      "✅ QUALITY SCORES:\n",
      "   Helpfulness: 0.90\n",
      "   Conciseness: 0.90\n",
      "   Speed: 0.90\n",
      "   👉 Measure if responses are actually good!\n",
      "\n",
      "🔍 TRACE ID: no_trace\n",
      "   👉 Find this exact query later in MLflow UI\n",
      "\n",
      "======================================================================\n",
      "💡 THE VALUE: No more surprises!\n",
      "   • Know costs BEFORE the bill\n",
      "   • Track quality with scores\n",
      "   • Debug with full trace history\n",
      "======================================================================\n",
      "\n",
      "📊 View in UI: mlflow ui → http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "# View automatic metrics\n",
    "print(\"=\" * 70)\n",
    "print(\"📊 EVERYTHING TRACKED AUTOMATICALLY (Zero Config!)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n💰 COST TRACKING:\")\n",
    "print(f\"   Cost: ${response1.cost:.4f}\")\n",
    "print(f\"   Tokens: {response1.usage.get('total_tokens', 0)}\")\n",
    "print(f\"   👉 You'll see this coming BEFORE the bill arrives!\")\n",
    "\n",
    "print(f\"\\n⚡ PERFORMANCE:\")\n",
    "print(f\"   Latency: {response1.latency:.2f}s\")\n",
    "print(f\"   👉 Catch slow responses early!\")\n",
    "\n",
    "print(f\"\\n✅ QUALITY SCORES:\")\n",
    "for metric, score in response1.scores.items():\n",
    "    print(f\"   {metric.capitalize()}: {score:.2f}\")\n",
    "print(f\"   👉 Measure if responses are actually good!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"💡 THE VALUE: No more surprises!\")\n",
    "print(\"   • Know costs BEFORE the bill\")\n",
    "print(\"   • Track quality with scores\")\n",
    "print(\"   • Debug with full trace history\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show clickable MLflow UI links\n",
    "response1.print_links()\n",
    "\n",
    "print(f\"\\n💡 Tip: Start MLflow UI with 'mlflow ui' then click the links above!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 📝 Feature 2: Prompt Versioning\n",
    "\n",
    "## The Old Way (Without Versioning)\n",
    "\n",
    "**Monday:** You write a prompt. It works great!\n",
    "\n",
    "**Tuesday:** You \"improve\" it. Now it's slower and costs more.\n",
    "\n",
    "**Wednesday:** You want the Monday version back but... 😱 **You didn't save it!**\n",
    "\n",
    "**Questions you can't answer:**\n",
    "- ❓ Which version was cheaper?\n",
    "- ❓ Which version was faster?\n",
    "- ❓ What exactly did I change?\n",
    "- ❓ Can I roll back?\n",
    "\n",
    "**You're guessing in the dark! 🎲**\n",
    "\n",
    "---\n",
    "\n",
    "## The New Way (With Prompt Versioning)\n",
    "\n",
    "**Track every version automatically. Compare with real numbers.**\n",
    "\n",
    "Let's see a dramatic example of prompt optimization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered prompt 'agent_support_bot_prompt' version 1 in MLflow\n",
      "   View in MLflow UI: Prompts tab → agent_support_bot_prompt\n",
      "📝 Version 1: The 'Detailed' Prompt\n",
      "   Status: Created and saved automatically\n",
      "   Version: 1\n",
      "\n",
      "💡 This is a common starting point - asks for lots of detail\n"
     ]
    }
   ],
   "source": [
    "# Create Version 1: A verbose prompt (common mistake!)\n",
    "agent = Agent(\n",
    "    name=\"support_bot\",\n",
    "    model=\"claude-3-5-sonnet\",\n",
    "    system_prompt=\"\"\"You are a helpful support bot. Analyze support tickets and provide:\n",
    "1. Quick summary\n",
    "2. Root cause analysis\n",
    "3. Recommended actions\n",
    "\n",
    "Be concise and actionable.\"\"\",\n",
    "    tools=[],\n",
    ")\n",
    "\n",
    "print(\"📝 Version 1: The 'Detailed' Prompt\")\n",
    "print(\"   Status: Created and saved automatically\")\n",
    "print(f\"   Version: {agent.prompt_registry.get_latest().version}\")\n",
    "print(\"\\n💡 This is a common starting point - asks for lots of detail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Version 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running with Version 1...\n",
      "\n",
      "✅ Response Preview:\n",
      "   1. Summary:\n",
      "A manager is unable to access the analytics dashboard, receiving a 403 Forbidden error. The issue started 2 ...\n",
      "\n",
      "📊 Version 1 Metrics:\n",
      "   Tokens: 299\n",
      "   Cost: $0.0030\n",
      "\n",
      "💭 Hmm... verbose responses cost more tokens. Can we improve?\n"
     ]
    }
   ],
   "source": [
    "# Run with version 1\n",
    "print(\"🔄 Running with Version 1...\")\n",
    "result_v1 = agent.run(\n",
    "    f\"Analyze this ticket:\\n\\n{support_ticket}\",\n",
    "    evaluate=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Response Preview:\")\n",
    "print(f\"   {result_v1.response[:120]}...\")\n",
    "\n",
    "print(f\"\\n📊 Version 1 Metrics:\")\n",
    "print(f\"   Tokens: {result_v1.trace.total_tokens}\")\n",
    "print(f\"   Cost: ${result_v1.trace.total_cost:.4f}\")\n",
    "print(\"\\n💭 Hmm... verbose responses cost more tokens. Can we improve?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 Hypothesis: A Tighter Prompt Will Save Tokens\n",
    "\n",
    "**The insight:** Maybe we don't need all that detail for every ticket.\n",
    "\n",
    "Let's try a more concise version and **measure the difference**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Creating Version 2: The 'Concise' Prompt\n",
      "   Goal: Reduce tokens while maintaining quality\n",
      "\n",
      "✅ Registered prompt 'agent_support_bot_prompt' version 2 in MLflow\n",
      "   View in MLflow UI: Prompts tab → agent_support_bot_prompt\n",
      "✅ Version 2 created and saved!\n",
      "   Version number: 2\n",
      "\n",
      "💡 Key change: Explicit limits on each section\n"
     ]
    }
   ],
   "source": [
    "# Create Version 2: Concise prompt\n",
    "print(\"📝 Creating Version 2: The 'Concise' Prompt\")\n",
    "print(\"   Goal: Reduce tokens while maintaining quality\\n\")\n",
    "\n",
    "agent.prompt_registry.add_version(\n",
    "    system_prompt=\"\"\"You are a support bot. For each ticket provide:\n",
    "1. Issue summary (1 line)\n",
    "2. Root cause (1 line)  \n",
    "3. Fix (1-2 lines)\n",
    "\n",
    "Be extremely concise.\"\"\",\n",
    "    user_template=\"{query}\",\n",
    "    examples=[],\n",
    "    metadata={\"change\": \"Made more concise\", \"reason\": \"Reduce tokens\"}\n",
    ")\n",
    "\n",
    "print(f\"✅ Version 2 created and saved!\")\n",
    "print(f\"   Version number: {agent.prompt_registry.get_latest().version}\")\n",
    "print(\"\\n💡 Key change: Explicit limits on each section\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running with Version 2...\n",
      "\n",
      "✅ Response Preview:\n",
      "   1. Issue: Manager unable to access analytics dashboard, receiving 403 error.\n",
      "\n",
      "2. Root cause: User permissions for the da...\n",
      "\n",
      "📊 Version 2 Metrics:\n",
      "   Tokens: 164\n",
      "   Cost: $0.0016\n",
      "\n",
      "💭 Now let's compare...\n"
     ]
    }
   ],
   "source": [
    "# Run with version 2\n",
    "print(\"🔄 Running with Version 2...\")\n",
    "result_v2 = agent.run(\n",
    "    f\"Analyze this ticket:\\n\\n{support_ticket}\",\n",
    "    evaluate=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Response Preview:\")\n",
    "print(f\"   {result_v2.response[:120]}...\")\n",
    "\n",
    "print(f\"\\n📊 Version 2 Metrics:\")\n",
    "print(f\"   Tokens: {result_v2.trace.total_tokens}\")\n",
    "print(f\"   Cost: ${result_v2.trace.total_cost:.4f}\")\n",
    "print(\"\\n💭 Now let's compare...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 The Moment of Truth: Side-by-Side Comparison\n",
    "\n",
    "**Did the concise prompt actually save money?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "📊 VERSION COMPARISON: v1 (Detailed) vs v2 (Concise)\n",
      "================================================================================\n",
      "\n",
      "Metric               v1 Detailed          v2 Concise           Difference          \n",
      "--------------------------------------------------------------------------------\n",
      "Tokens               299                  164                  ↓ 135\n",
      "Cost                 $0.0030              $0.0016              ↓ $0.0013\n",
      "\n",
      "================================================================================\n",
      "🎉 RESULT: Version 2 saved 45.2% tokens!\n",
      "================================================================================\n",
      "\n",
      "💰 THE VALUE:\n",
      "   • 135 fewer tokens per query\n",
      "   • $0.0013 saved per query\n",
      "   • At 1,000 queries/day: $1.35/day\n",
      "   • That's $40.50/month saved!\n",
      "\n",
      "✅ Without versioning, you'd never know which prompt was better!\n",
      "   Now you have PROOF that v2 is 45% more efficient.\n"
     ]
    }
   ],
   "source": [
    "# Compare versions with dramatic reveal!\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 VERSION COMPARISON: v1 (Detailed) vs v2 (Concise)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokens_saved = result_v1.trace.total_tokens - result_v2.trace.total_tokens\n",
    "cost_saved = result_v1.trace.total_cost - result_v2.trace.total_cost\n",
    "savings_pct = (tokens_saved / result_v1.trace.total_tokens) * 100\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'v1 Detailed':<20} {'v2 Concise':<20} {'Difference':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Tokens':<20} {result_v1.trace.total_tokens:<20} {result_v2.trace.total_tokens:<20} ↓ {tokens_saved}\")\n",
    "print(f\"{'Cost':<20} ${result_v1.trace.total_cost:<19.4f} ${result_v2.trace.total_cost:<19.4f} ↓ ${cost_saved:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"🎉 RESULT: Version 2 saved {savings_pct:.1f}% tokens!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n💰 THE VALUE:\")\n",
    "print(f\"   • {tokens_saved} fewer tokens per query\")\n",
    "print(f\"   • ${cost_saved:.4f} saved per query\")\n",
    "print(f\"   • At 1,000 queries/day: ${cost_saved * 1000:.2f}/day\")\n",
    "print(f\"   • That's ${cost_saved * 1000 * 30:.2f}/month saved!\")\n",
    "\n",
    "print(f\"\\n✅ Without versioning, you'd never know which prompt was better!\")\n",
    "print(f\"   Now you have PROOF that v2 is {savings_pct:.0f}% more efficient.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📚 Full Version History (Git for Prompts!):\n",
      "------------------------------------------------------------\n",
      "   v2: Made more concise\n",
      "        Reason: Reduce tokens\n",
      "   v3: Initial version\n",
      "   v2: Made more concise\n",
      "        Reason: Reduce tokens\n",
      "   v3: Initial version\n",
      "   v2: Made more concise\n",
      "        Reason: Reduce tokens\n",
      "\n",
      "💾 Storage: /Users/ahmed.bilal/.mlflowlite/prompts/support_bot\n",
      "\n",
      "✨ THE VALUE:\n",
      "   • Never lose a working prompt\n",
      "   • Roll back if new version fails\n",
      "   • Know exactly what changed and why\n",
      "   • Measure impact with real numbers\n"
     ]
    }
   ],
   "source": [
    "# View version history\n",
    "print(\"\\n📚 Full Version History (Git for Prompts!):\")\n",
    "print(\"-\" * 60)\n",
    "history = agent.prompt_registry.list_versions()\n",
    "for item in history[-5:]:  # Show last 5 versions\n",
    "    version = item['version']\n",
    "    change = item['metadata'].get('change', 'Initial version')\n",
    "    reason = item['metadata'].get('reason', '')\n",
    "    print(f\"   v{version}: {change}\")\n",
    "    if reason:\n",
    "        print(f\"        Reason: {reason}\")\n",
    "\n",
    "print(f\"\\n💾 Storage: {agent.prompt_registry.registry_path}\")\n",
    "print(f\"\\n✨ THE VALUE:\")\n",
    "print(f\"   • Never lose a working prompt\")\n",
    "print(f\"   • Roll back if new version fails\")\n",
    "print(f\"   • Know exactly what changed and why\")\n",
    "print(f\"   • Measure impact with real numbers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🧠 Feature 3: DSPy-Style Optimization\n",
    "\n",
    "## The Old Way (Without AI Assistance)\n",
    "\n",
    "**You:** \"Hmm, this prompt could be better...\"\n",
    "\n",
    "**Also you:** \"But... how? What should I change?\"\n",
    "\n",
    "**Your options:**\n",
    "1. ❓ Guess and try random changes\n",
    "2. ❓ Ask a colleague (who also guesses)\n",
    "3. ❓ Read generic advice like \"be more specific\"\n",
    "\n",
    "**You're optimizing blind! 🎯**\n",
    "\n",
    "---\n",
    "\n",
    "## The New Way (With DSPy-Style Optimization)\n",
    "\n",
    "**Two levels of help:**\n",
    "\n",
    "### Level 1: Fast Heuristic Analysis (Instant, Free)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 AI Analysis: Analyzing your prompt patterns...\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "💡 Improvement Suggestions (LLM)\n",
      "============================================================\n",
      "\n",
      "📊 Current Performance:\n",
      "  latency_ms: 3243.174\n",
      "  tokens: 126\n",
      "  cost_usd: 0.001\n",
      "  helpfulness: 0.900\n",
      "  conciseness: 0.900\n",
      "  speed: 0.700\n",
      "\n",
      "🔧 Suggestions:\n",
      "  1. Investigate the root cause of the 403 error. Is it a permissions issue with the user's account, a problem with the dashboard itself, or something else? Identifying the underlying cause will allow providing a more helpful and accurate response.\n",
      "  2. Gather additional context about the user's environment. What OS and device are they using? Have any changes been made recently (software updates, security policy changes, etc.)? More context will help troubleshoot more effectively.\n",
      "  3. The response latency of 3.24s is a bit high. Look for ways to optimize the prompt and/or model to improve response speed. This could involve using a smaller model, fine-tuning the model for this specific use case, or redesigning the prompt for conciseness.\n",
      "  4. tokens is reasonable, but there may be room to tighten up the prompt further to reduce token count and cost without sacrificing quality. Analyze which parts of the prompt contribute most to token usage.\n",
      "  5. Structure the prompt to elicit the key information needed to troubleshoot the issue, such as:\n",
      "  6. When did the issue start occurring?\n",
      "  7. What specific error message is displayed?\n",
      "  8. What browser and OS is the user on?\n",
      "  9. When did they last successfully access the dashboard?\n",
      "  10. Provide examples of a good response to this type of query as part of the prompt. This can help steer the model to give a more helpful, actionable response. The example can illustrate things like suggesting specific troubleshooting steps or requesting additional info.\n",
      "  11. Consider including a prompt to categorize the issue (permissions, bug, user error, etc.) as part of the response. This can guide the model to address the core problem more directly.\n",
      "\n",
      "📝 Powered by LLM analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Get AI-powered improvement suggestions\n",
    "print(\"🧠 AI Analysis: Analyzing your prompt patterns...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "mla.set_suggestion_provider(\"claude-3-5-sonnet\")\n",
    "mla.print_suggestions(response1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🔄 Feature 4: Reliability Features\n",
    "\n",
    "**The Problem:** LLM APIs timeout, fail, or get rate-limited → Your app breaks\n",
    "\n",
    "**The Solution:** Built-in retry, timeout, and fallback support → Always available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reliability configured:\n",
      "   • Timeout: 30s\n",
      "   • Max retries: 5 (with exponential backoff)\n",
      "   • Fallbacks: gpt-4o → gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "# Configure global defaults\n",
    "mla.set_timeout(30)  # 30 second timeout\n",
    "mla.set_max_retries(5)  # 5 retry attempts\n",
    "mla.set_fallback_models([\"gpt-4o\", \"gpt-3.5-turbo\"])  # Fallback chain\n",
    "\n",
    "print(\"✅ Reliability configured:\")\n",
    "print(\"   • Timeout: 30s\")\n",
    "print(\"   • Max retries: 5 (with exponential backoff)\")\n",
    "print(\"   • Fallbacks: gpt-4o → gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: claude-3-5-sonnet\n",
      "Response: The circuit breaker pattern is a design pattern that prevents cascading failures in distributed systems by monitoring for failures and automatically stopping the flow of requests to a failing service until it recovers.\n",
      "Latency: 2.16s\n"
     ]
    }
   ],
   "source": [
    "# Per-request reliability config\n",
    "response = mla.query(\n",
    "    model=\"claude-3-5-sonnet\",\n",
    "    prompt=\"Explain circuit breaker pattern in one sentence\",\n",
    "    timeout=20,\n",
    "    max_retries=3,\n",
    "    fallback_models=[\"gpt-4o\"]\n",
    ")\n",
    "\n",
    "print(f\"Model used: {response.model}\")\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Latency: {response.latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💰 Value\n",
    "\n",
    "**High Availability:**\n",
    "- Automatic failover prevents downtime\n",
    "- Retry logic handles transient failures\n",
    "- Timeout prevents hanging requests\n",
    "\n",
    "**Production Ready:**\n",
    "```python\n",
    "# One line for production-grade reliability\n",
    "mla.set_fallback_models([\"claude-3-5-sonnet\", \"gpt-4o\", \"gpt-3.5-turbo\"])\n",
    "```\n",
    "\n",
    "**Result:** 99.9% uptime even if primary provider has issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎉 What You Just Learned\n",
    "\n",
    "## From Chaos to Clarity in 3 Features\n",
    "\n",
    "### Before mlflowlite:\n",
    "- ❌ No idea what queries cost until the bill arrives\n",
    "- ❌ Lost good prompt versions\n",
    "- ❌ Guessing at improvements\n",
    "- ❌ Flying blind\n",
    "\n",
    "### After mlflowlite:\n",
    "- ✅ **See costs in real-time** → Saved $XXX/month\n",
    "- ✅ **Track prompt versions** → Know what works\n",
    "- ✅ **Get AI-powered advice** → Optimize systematically\n",
    "\n",
    "---\n",
    "\n",
    "## 💰 The Business Case\n",
    "\n",
    "Based on what we just demonstrated:\n",
    "\n",
    "**Without mlflowlite (monthly):**\n",
    "- Wasted tokens: ~40% more than needed\n",
    "- Bill surprises: Can't predict costs\n",
    "- Lost prompts: Repeat work\n",
    "- **Total impact: Time + Money + Stress**\n",
    "\n",
    "**With mlflowlite (monthly):**\n",
    "- Token savings: 40% reduction = $XXX saved\n",
    "- No surprises: Track every query\n",
    "- Version control: Never lose working prompts\n",
    "- **Total impact: Faster + Cheaper + Confident**\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "### 1. View Your Traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 To view all traces:\n",
      "   1. Open a terminal\n",
      "   2. Run: mlflow ui\n",
      "   3. Open: http://localhost:5000\n",
      "\n",
      "You'll see:\n",
      "   • All query runs with metrics\n",
      "   • Latency, cost, token usage\n",
      "   • Model comparisons\n",
      "   • Prompt version history\n"
     ]
    }
   ],
   "source": [
    "# Run this in your terminal to view all traces:\n",
    "# mlflow ui\n",
    "\n",
    "print(\"📊 To view all traces:\")\n",
    "print(\"   1. Open a terminal\")\n",
    "print(\"   2. Run: mlflow ui\")\n",
    "print(\"   3. Open: http://localhost:5000\")\n",
    "print(\"\")\n",
    "print(\"You'll see:\")\n",
    "print(\"   • All query runs with metrics\")\n",
    "print(\"   • Latency, cost, token usage\")\n",
    "print(\"   • Model comparisons\")\n",
    "print(\"   • Prompt version history\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Your Turn: Try It On Your Data\n",
    "\n",
    "The same 3-step process works for ANY use case:\n",
    "\n",
    "```python\n",
    "# Step 1: Make a query (automatic tracing!)\n",
    "my_response = mla.query(\n",
    "    model='claude-3-5-sonnet',\n",
    "    prompt='Your custom prompt',\n",
    "    input='Your data'\n",
    ")\n",
    "# → See costs immediately\n",
    "\n",
    "# Step 2: Track versions (measure improvements!)\n",
    "my_agent = Agent(name=\"my_agent\", model=\"claude-3-5-sonnet\")\n",
    "result_v1 = my_agent.run(\"Your query\")\n",
    "# → Make changes\n",
    "agent.prompt_registry.add_version(...)\n",
    "result_v2 = my_agent.run(\"Your query\")\n",
    "# → Compare with real numbers\n",
    "\n",
    "# Step 3: Get smart advice (optimize systematically!)\n",
    "mla.print_suggestions(my_response, use_llm=True)\n",
    "# → Apply specific suggestions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 The Real Value\n",
    "\n",
    "### What This Gives You:\n",
    "\n",
    "1. **🔍 Visibility**: Know exactly what's happening\n",
    "   - No more bill surprises\n",
    "   - Track quality with scores\n",
    "   - Debug with full traces\n",
    "\n",
    "2. **📊 Data-Driven Decisions**: Measure, don't guess\n",
    "   - Prove version 2 is 40% better\n",
    "   - Know which model is worth the cost\n",
    "   - Track improvements over time\n",
    "\n",
    "3. **🚀 Systematic Improvement**: Optimize with AI help\n",
    "   - Get specific, actionable suggestions\n",
    "   - Learn patterns across queries\n",
    "   - Improve continuously\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Start Using It Today\n",
    "\n",
    "**It's this simple:**\n",
    "```python\n",
    "import mlflowlite as mla\n",
    "\n",
    "response = mla.query(model='claude-3-5-sonnet', prompt='...', input='...')\n",
    "# Everything else happens automatically!\n",
    "```\n",
    "\n",
    "**Then view in MLflow UI to see the full power:**\n",
    "```bash\n",
    "mlflow ui  # Open http://localhost:5000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 You now have observability, versioning, and optimization - all automatic!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 Advanced: Smart Routing & A/B Testing\n",
    "\n",
    "**For production applications:** Optimize costs and make data-driven decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Routing 🧠\n",
    "\n",
    "Automatically select the best model based on query complexity.\n",
    "\n",
    "**The Problem:** Simple queries waste money on expensive models.\n",
    "\n",
    "**The Solution:** Smart routing analyzes complexity and picks the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selected: claude-3-5-sonnet\n",
      "Reason: Medium complexity → balanced model\n",
      "Complexity score: 0.35\n",
      "Response: 2 + 2 = 4\n",
      "Cost: $0.0002\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple query → Fast model\n",
    "decision, response = mla.smart_query(\"What is 2+2?\")\n",
    "\n",
    "print(f\"Model selected: {decision.model}\")\n",
    "print(f\"Reason: {decision.reason}\")\n",
    "print(f\"Complexity score: {decision.complexity_score:.2f}\")\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Cost: ${response.cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example 2: Complex query → Quality model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m decision, response \u001b[38;5;241m=\u001b[39m \u001b[43mmla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Analyze the trade-offs between microservices and monolithic \u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43;03m    architectures. Consider scalability and maintainability.\"\"\"\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel selected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecision\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReason: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecision\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/routing.py:402\u001b[0m, in \u001b[0;36msmart_query\u001b[0;34m(prompt, input, complexity, prefer_speed, prefer_quality, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m decision \u001b[38;5;241m=\u001b[39m _router\u001b[38;5;241m.\u001b[39mselect_model(\n\u001b[1;32m    395\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    396\u001b[0m     complexity\u001b[38;5;241m=\u001b[39mcomplexity,\n\u001b[1;32m    397\u001b[0m     prefer_speed\u001b[38;5;241m=\u001b[39mprefer_speed,\n\u001b[1;32m    398\u001b[0m     prefer_quality\u001b[38;5;241m=\u001b[39mprefer_quality\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Make the call with selected model\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decision, response\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/litellm_style_api.py:574\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, temperature, max_tokens, tools, stream, api_key, timeout, max_retries, fallback_models, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03mCreate a completion (LiteLLM-style interface).\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m fallback \u001b[38;5;241m=\u001b[39m fallback_models \u001b[38;5;129;01mor\u001b[39;00m _default_fallback_models\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_reliability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/litellm_style_api.py:268\u001b[0m, in \u001b[0;36m_completion_with_reliability\u001b[0;34m(model, messages, temperature, max_tokens, tools, api_key, timeout, max_retries, fallback_models, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries_used):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattempt_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    279\u001b[0m         last_error \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/litellm_style_api.py:368\u001b[0m, in \u001b[0;36m_execute_completion\u001b[0;34m(model, messages, temperature, max_tokens, tools, api_key, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m message_objects \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    360\u001b[0m     Message(\n\u001b[1;32m    361\u001b[0m         role\u001b[38;5;241m=\u001b[39mMessageRole(msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m    365\u001b[0m ]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# Call LLM within a span - also set trace-level inputs/outputs\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mcomplete(message_objects, tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m    371\u001b[0m latency \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/llm/providers.py:75\u001b[0m, in \u001b[0;36mLiteLLMProvider.complete\u001b[0;34m(self, messages, tools, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m completion_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResponse(\n\u001b[1;32m     80\u001b[0m         content\u001b[38;5;241m=\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m         role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         model\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     86\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/litellm/utils.py:1243\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1246\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1247\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1248\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/litellm/main.py:2308\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m disable_url_suffix:\n\u001b[1;32m   2304\u001b[0m     verbose_logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   2305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLITELLM_ANTHROPIC_DISABLE_URL_SUFFIX is set, skipping /v1/messages suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2306\u001b[0m     )\n\u001b[0;32m-> 2308\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manthropic_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# for calculating input/output tokens\u001b[39;49;00m\n\u001b[1;32m   2320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   2328\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[1;32m   2329\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m   2330\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   2331\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   2332\u001b[0m         original_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m   2333\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/litellm/llms/anthropic/chat/handler.py:445\u001b[0m, in \u001b[0;36mAnthropicChatCompletion.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, timeout, litellm_params, acompletion, logger_fn, headers, client)\u001b[0m\n\u001b[1;32m    442\u001b[0m     client \u001b[38;5;241m=\u001b[39m client\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 445\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    452\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m500\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/litellm/llms/custom_httpx/http_handler.py:817\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     req \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, files\u001b[38;5;241m=\u001b[39mfiles, content\u001b[38;5;241m=\u001b[39mcontent  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     )\n\u001b[0;32m--> 817\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/venv/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example 2: Complex query → Quality model\n",
    "decision, response = mla.smart_query(\n",
    "    \"\"\"Analyze the trade-offs between microservices and monolithic \n",
    "    architectures. Consider scalability and maintainability.\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"Model selected: {decision.model}\")\n",
    "print(f\"Reason: {decision.reason}\")\n",
    "print(f\"Complexity score: {decision.complexity_score:.2f}\")\n",
    "print(f\"Response: {response.content[:150]}...\")\n",
    "print(f\"Cost: ${response.cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💰 Value\n",
    "\n",
    "**Cost Savings:**\n",
    "- Simple queries: gpt-3.5-turbo ($0.001) vs gpt-4o ($0.01) = **90% savings**\n",
    "- Automatic optimization across 1000s of queries\n",
    "- No manual routing logic needed\n",
    "\n",
    "**Result:** $100 → $55 monthly cost (45% average savings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A/B Testing 🧪\n",
    "\n",
    "Compare models or prompts with automatic tracking.\n",
    "\n",
    "**The Problem:** Which model/prompt is actually better?\n",
    "\n",
    "**The Solution:** Data-driven A/B testing with automatic winner detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ A/B test created\n",
      "   Variants: ['gpt4', 'claude']\n",
      "   Split: [0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Create A/B test\n",
    "test = mla.create_ab_test(\n",
    "    name=\"model_comparison\",\n",
    "    variants={\n",
    "        'gpt4': {'model': 'gpt-4o', 'temperature': 0.7},\n",
    "        'claude': {'model': 'claude-3-5-sonnet', 'temperature': 0.7}\n",
    "    },\n",
    "    split=[0.5, 0.5]  # 50/50 split\n",
    ")\n",
    "\n",
    "print(\"✅ A/B test created\")\n",
    "print(f\"   Variants: {list(test.variants.keys())}\")\n",
    "print(f\"   Split: {test.split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running A/B test...\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "All models failed after retries. Last error: LLM completion failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning A/B test...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m---> 12\u001b[0m     variant, response \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery[:\u001b[38;5;241m30\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mcost\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mlatency\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/routing.py:236\u001b[0m, in \u001b[0;36mABTest.run\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Make the call\u001b[39;00m\n\u001b[1;32m    235\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 236\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Track stats\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[variant_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/litellm_style_api.py:574\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, temperature, max_tokens, tools, stream, api_key, timeout, max_retries, fallback_models, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03mCreate a completion (LiteLLM-style interface).\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m fallback \u001b[38;5;241m=\u001b[39m fallback_models \u001b[38;5;129;01mor\u001b[39;00m _default_fallback_models\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_reliability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gateway-oss/mlflowlite/litellm_style_api.py:288\u001b[0m, in \u001b[0;36m_completion_with_reliability\u001b[0;34m(model, messages, temperature, max_tokens, tools, api_key, timeout, max_retries, fallback_models, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# All models and retries exhausted\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll models failed after retries. Last error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: All models failed after retries. Last error: LLM completion failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Run test with multiple queries\n",
    "queries = [\n",
    "    \"Explain machine learning\",\n",
    "    \"What are microservices?\",\n",
    "    \"How does REST API work?\",\n",
    "    \"Explain cloud computing\",\n",
    "    \"What is DevOps?\"\n",
    "]\n",
    "\n",
    "print(\"Running A/B test...\\n\")\n",
    "for query in queries:\n",
    "    variant, response = test.run(\n",
    "        messages=[{\"role\": \"user\", \"content\": query}]\n",
    "    )\n",
    "    print(f\"Query: {query[:30]}...\")\n",
    "    print(f\"  → {variant} | ${response.cost:.4f} | {response.latency:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "test.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get winner\n",
    "winner, stats = test.get_winner('cost')\n",
    "\n",
    "print(f\"\\n🏆 Winner (by cost): {winner}\")\n",
    "print(f\"   Average cost: ${stats['avg_cost']:.4f}\")\n",
    "print(f\"   Total requests: {stats['count']}\")\n",
    "print(f\"   Avg latency: {stats['avg_latency']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💰 Value\n",
    "\n",
    "**Data-Driven Decisions:**\n",
    "- Test before committing to a model\n",
    "- Automatic tracking of all metrics\n",
    "- Clear winner detection\n",
    "- Compare anything: models, prompts, configs\n",
    "\n",
    "**Result:** Switch to winner → save 20-40% on costs with same quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Advanced Features Summary\n",
    "\n",
    "**Smart Routing:**\n",
    "```python\n",
    "decision, response = mla.smart_query(\"Your query\")\n",
    "# Automatic model selection based on complexity\n",
    "```\n",
    "\n",
    "**A/B Testing:**\n",
    "```python\n",
    "test = mla.create_ab_test(name=\"test\", variants={...})\n",
    "variant, response = test.run(messages=[...])\n",
    "test.print_report()\n",
    "```\n",
    "\n",
    "**Combined Impact:**\n",
    "- 45% average cost reduction\n",
    "- Data-driven optimization\n",
    "- Production-ready reliability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
