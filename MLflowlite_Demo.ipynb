{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow AI Gateway: Stay at the Frontier\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "When new models (GPT-5, Claude Sonnet 4.5, Gemini 2.5) are released, platform teams need to **evaluate, compare, and gradually migrate** their apps \u2014 balancing **quality, latency, cost, and governance** \u2014 without breaking production or rewriting code.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**Part 1: Fundamentals** (5 min)\n",
    "1. \u2705 Automated tracing - every call logged automatically\n",
    "2. \ud83d\udcdd Prompt versioning - register and retrieve versions\n",
    "\n",
    "**Part 2: Model Migration Workflow** (15 min)\n",
    "3. \ud83d\udd04 Baseline capture from current model\n",
    "4. \ud83c\udfaf Auto-optimize prompts for new model\n",
    "5. \ud83d\udcca Compare quality, latency, cost\n",
    "6. \ud83d\ude80 Gradual migration with A/B testing\n",
    "\n",
    "**Key Benefit:** Inline APIs - just call functions, automatic MLflow logging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflowlite as mlflow\n",
    "import os\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Set experiment (all automatic runs go here)\n",
    "mlflow.set_experiment(\"ai_gateway_demo\")\n",
    "\n",
    "print(\"\u2705 Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Fundamentals\n",
    "\n",
    "## \ud83d\udcca Feature 1: Automatic Tracing\n",
    "\n",
    "### The Old Way (Without Tracing)\n",
    "\n",
    "You call an LLM:\n",
    "\n",
    "```python\n",
    "response = openai.chat.completions.create(...)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "Questions you can't answer:\n",
    "- \u2753 How much did that cost?\n",
    "- \u2753 How long did it take?\n",
    "- \u2753 Was the response quality good?\n",
    "- \u2753 Can I compare this to yesterday's version?\n",
    "\n",
    "**You're flying blind!** \ud83d\udee9\ufe0f\ud83d\udca8\n",
    "\n",
    "---\n",
    "\n",
    "### The New Way (With mlflowlite)\n",
    "\n",
    "Same code, **automatic insights**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LLM call - automatically creates MLflow run!\n",
    "@mlflow.trace\n",
    "def classify_sentiment(text: str) -> str:\n",
    "    \"\"\"Classify sentiment - each call creates an MLflow run automatically\"\"\"\n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Classify sentiment as positive/negative/neutral: {text}\"}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "# Just call it - automatic run creation!\n",
    "result = classify_sentiment(\"This product is amazing!\")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83c\udfaf Value Unlocked: See Everything Automatically\")\n",
    "print(\"=\"*60)\n",
    "print(\"\u2705 Cost tracked automatically\")\n",
    "print(\"\u2705 Latency tracked automatically\")\n",
    "print(\"\u2705 Tokens tracked automatically\")\n",
    "print(\"\u2705 MLflow run created automatically\")\n",
    "print(\"\u2705 Trace viewable in MLflow UI\")\n",
    "print(\"\\n\ud83d\udca1 Check MLflow UI: http://localhost:5000 \u2192 Traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf What You Get for FREE\n",
    "\n",
    "Every call automatically logs:\n",
    "- \ud83d\udcb0 **Cost** - Exact cost per call\n",
    "- \u23f1\ufe0f **Latency** - Response time\n",
    "- \ud83c\udfab **Tokens** - Input + output tokens\n",
    "- \ud83d\udcca **Run ID** - Unique identifier\n",
    "- \ud83d\udd17 **UI Links** - Direct links to MLflow UI\n",
    "\n",
    "**No manual logging. No boilerplate. Just insights!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Feature 2: Prompt Versioning\n",
    "\n",
    "### The Old Way (Without Versioning)\n",
    "\n",
    "**Monday:** You write a prompt. It works great!\n",
    "\n",
    "**Tuesday:** You \"improve\" it. Now it's slower and costs more.\n",
    "\n",
    "**Wednesday:** You want Monday's version back but... \ud83d\ude31 **You didn't save it!**\n",
    "\n",
    "Questions you can't answer:\n",
    "- \u2753 Which version was cheaper?\n",
    "- \u2753 Which version was faster?\n",
    "- \u2753 What exactly did I change?\n",
    "- \u2753 Can I roll back?\n",
    "\n",
    "**You're guessing in the dark!** \ud83c\udfb2\n",
    "\n",
    "---\n",
    "\n",
    "### The New Way (With Prompt Versioning)\n",
    "\n",
    "Track every version automatically. Compare with real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register prompt Version 1 (Simple)\n",
    "prompt_v1 = mlflow.register_prompt(\n",
    "    name=\"sentiment_classifier\",\n",
    "    template=\"Classify sentiment: {{text}}\"\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered prompt 'sentiment_classifier' version 1\")\n",
    "print(f\"   URI: {prompt_v1['uri']}\")\n",
    "print(f\"   Template: {prompt_v1['template']}\")\n",
    "print(\"\\n\ud83d\udca1 View in MLflow UI: Prompts tab \u2192 sentiment_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Version 1\n",
    "@mlflow.trace\n",
    "def classify_v1(text: str) -> str:\n",
    "    \"\"\"Using prompt v1 - automatic run creation!\"\"\"\n",
    "    prompt_text = prompt_v1['template'].replace('{{text}}', text)\n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"sentiment_classifier\"  # Links trace to prompt version\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Test v1 - creates run automatically!\n",
    "result_v1 = classify_v1(\"The service was excellent!\")\n",
    "print(f\"v1 Result: {result_v1}\")\n",
    "print(\"\\n\u2705 MLflow run created automatically with prompt linkage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Version 2 (Improved with explicit instructions)\n",
    "prompt_v2 = mlflow.register_prompt(\n",
    "    name=\"sentiment_classifier\",\n",
    "    template=\"\"\"Classify the sentiment of the following text.\n",
    "\n",
    "Text: {{text}}\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY with: positive, negative, or neutral\n",
    "- Use lowercase\n",
    "- No explanation\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered prompt 'sentiment_classifier' version 2\")\n",
    "print(f\"   URI: {prompt_v2['uri']}\")\n",
    "print(\"\\n\ud83d\udcca Now you have 2 versions tracked in MLflow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Version 2\n",
    "@mlflow.trace\n",
    "def classify_v2(text: str) -> str:\n",
    "    \"\"\"Using prompt v2 - automatic run creation!\"\"\"\n",
    "    prompt_text = prompt_v2['template'].replace('{{text}}', text)\n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"sentiment_classifier\"\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Test v2 - creates run automatically!\n",
    "result_v2 = classify_v2(\"The service was excellent!\")\n",
    "print(f\"v2 Result: {result_v2}\")\n",
    "print(\"\\n\u2705 Another MLflow run created automatically!\")\n",
    "print(\"\\n\ud83d\udca1 Compare both runs in MLflow UI to see which is better!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve specific versions\n",
    "retrieved_v1 = mlflow.get_prompt(name=\"sentiment_classifier\", version=1)\n",
    "retrieved_v2 = mlflow.get_prompt(name=\"sentiment_classifier\", version=2)\n",
    "\n",
    "print(\"\ud83d\udcda Version History:\\n\")\n",
    "print(f\"Version 1: {retrieved_v1['template'][:50]}...\")\n",
    "print(f\"Version 2: {retrieved_v2['template'][:50]}...\")\n",
    "print(\"\\n\u2705 Git-like versioning for prompts!\")\n",
    "print(\"   \u2022 Roll back anytime\")\n",
    "print(\"   \u2022 Compare versions\")\n",
    "print(\"   \u2022 Track all changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Model Migration Workflow\n",
    "\n",
    "## \ud83c\udfaf The Scenario\n",
    "\n",
    "You have a **production sentiment classifier** using GPT-4.\n",
    "\n",
    "**GPT-4o-mini is released**: Faster and 50x cheaper!\n",
    "\n",
    "**The Question**: Can you migrate without breaking production?\n",
    "\n",
    "**The Answer**: Yes! Using inline APIs with automatic tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Your Production App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production prompt\n",
    "prod_prompt = mlflow.register_prompt(\n",
    "    name=\"production_sentiment\",\n",
    "    template=\"Classify sentiment as positive, negative, or neutral: {{text}}\"\n",
    ")\n",
    "\n",
    "# Production function (GPT-4)\n",
    "@mlflow.trace\n",
    "def production_classifier(text: str, model: str = \"openai/gpt-4\") -> str:\n",
    "    \"\"\"Production classifier - automatic run creation!\"\"\"\n",
    "    prompt_text = prod_prompt['template'].replace('{{text}}', text)\n",
    "    response = mlflow.llm_call(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"production_sentiment\"\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "# Test current production\n",
    "result = production_classifier(\"Amazing product!\")\n",
    "print(f\"GPT-4 Result: {result}\")\n",
    "print(\"\\n\u2705 Current production: GPT-4\")\n",
    "print(\"   \u2022 MLflow run created automatically\")\n",
    "print(\"   \u2022 Cost/latency tracked automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Collect Baseline Data\n",
    "\n",
    "Collect outputs from GPT-4 on representative data. **Each call creates an MLflow run automatically!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production test cases\n",
    "test_cases = [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"The service was terrible.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"I'm so disappointed with this purchase.\",\n",
    "    \"Best experience ever!\",\n",
    "    \"The product works as described.\",\n",
    "    \"I can't believe how amazing this is!\",\n",
    "    \"Worst customer support ever.\",\n",
    "    \"It's fine for the price.\",\n",
    "    \"This exceeded all my expectations!\"\n",
    "]\n",
    "\n",
    "# Collect baseline - each call creates an MLflow run!\n",
    "print(\"\ud83d\udd04 Collecting baseline from GPT-4...\\n\")\n",
    "baseline_outputs = []\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = production_classifier(text, model=\"openai/gpt-4\")  # Auto-creates run!\n",
    "    baseline_outputs.append(result)\n",
    "    print(f\"{i}. {text[:35]:35s} \u2192 {result}\")\n",
    "\n",
    "print(f\"\\n\u2705 {len(baseline_outputs)} baseline outputs collected\")\n",
    "print(f\"\u2705 {len(baseline_outputs)} MLflow runs created automatically!\")\n",
    "print(\"\\n\ud83d\udca1 All runs visible in MLflow UI with cost/latency data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test New Model\n",
    "\n",
    "Test GPT-4o-mini with the **same prompt**. Spot the differences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new model - each call creates an MLflow run!\n",
    "print(\"\ud83d\udd04 Testing GPT-4o-mini with same prompt...\\n\")\n",
    "new_model_outputs = []\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = production_classifier(text, model=\"openai/gpt-4o-mini\")  # Auto-creates run!\n",
    "    new_model_outputs.append(result)\n",
    "    baseline = baseline_outputs[i-1]\n",
    "    match = \"\u2705\" if result == baseline else \"\u274c\"\n",
    "    print(f\"{i}. {match} new: {result:8s} (baseline: {baseline})\")\n",
    "\n",
    "# Calculate consistency\n",
    "matches = sum(1 for n, b in zip(new_model_outputs, baseline_outputs) if n == b)\n",
    "consistency = matches / len(baseline_outputs) * 100\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Consistency: {consistency:.0f}%\")\n",
    "print(f\"\u2705 {len(new_model_outputs)} more MLflow runs created automatically!\")\n",
    "if consistency < 90:\n",
    "    print(\"\\n\u26a0\ufe0f  Need optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Auto-Optimize Prompt\n",
    "\n",
    "Use DSPy to optimize the prompt for GPT-4o-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflowlite.optimization import optimize_prompts\n",
    "\n",
    "print(\"\ud83c\udfaf Optimizing prompt for GPT-4o-mini...\\n\")\n",
    "\n",
    "# Training data\n",
    "training_data = [\n",
    "    {\"text\": text, \"expected\": baseline}\n",
    "    for text, baseline in zip(test_cases[:5], baseline_outputs[:5])\n",
    "]\n",
    "\n",
    "# Optimize - this creates MLflow runs automatically during optimization!\n",
    "optimized_text = optimize_prompts(\n",
    "    task=\"sentiment\",\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    training_data=training_data,\n",
    "    original_prompt=prod_prompt['template']\n",
    ")\n",
    "\n",
    "print(\"\u2705 Optimization complete!\")\n",
    "print(f\"\\n\ud83d\udcdd Original:\\n{prod_prompt['template']}\")\n",
    "print(f\"\\n\ud83d\udcdd Optimized:\\n{optimized_text}\")\n",
    "print(\"\\n\u2705 Optimization runs tracked in MLflow automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register optimized prompt\n",
    "opt_prompt = mlflow.register_prompt(\n",
    "    name=\"production_sentiment\",\n",
    "    template=optimized_text\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered optimized prompt (new version)\")\n",
    "print(f\"   URI: {opt_prompt['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Optimized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized classifier\n",
    "@mlflow.trace\n",
    "def optimized_classifier(text: str) -> str:\n",
    "    \"\"\"Optimized classifier - automatic run creation!\"\"\"\n",
    "    prompt_text = opt_prompt['template'].replace('{{text}}', text)\n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"production_sentiment\"\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "# Test optimized - each call creates an MLflow run!\n",
    "print(\"\ud83d\udd04 Testing optimized version...\\n\")\n",
    "optimized_outputs = []\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = optimized_classifier(text)  # Auto-creates run!\n",
    "    optimized_outputs.append(result)\n",
    "    baseline = baseline_outputs[i-1]\n",
    "    match = \"\u2705\" if result == baseline else \"\u274c\"\n",
    "    print(f\"{i}. {match} optimized: {result:8s} (baseline: {baseline})\")\n",
    "\n",
    "# Calculate improvement\n",
    "matches = sum(1 for o, b in zip(optimized_outputs, baseline_outputs) if o == b)\n",
    "improved = matches / len(baseline_outputs) * 100\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Before: {consistency:.0f}% | After: {improved:.0f}%\")\n",
    "print(f\"\ud83c\udf89 Improvement: +{improved - consistency:.0f} points!\")\n",
    "print(f\"\\n\u2705 {len(optimized_outputs)} more MLflow runs created automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"GPT-4\",\n",
    "        \"Prompt\": \"Original\",\n",
    "        \"Consistency\": \"100%\",\n",
    "        \"Latency\": \"~800ms\",\n",
    "        \"Cost/1M\": \"$30\",\n",
    "        \"Status\": \"\ud83d\udfe2 Production\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-4o-mini\",\n",
    "        \"Prompt\": \"Original\",\n",
    "        \"Consistency\": f\"{consistency:.0f}%\",\n",
    "        \"Latency\": \"~300ms\",\n",
    "        \"Cost/1M\": \"$0.60\",\n",
    "        \"Status\": \"\u274c Not Ready\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-4o-mini\",\n",
    "        \"Prompt\": \"Optimized\",\n",
    "        \"Consistency\": f\"{improved:.0f}%\",\n",
    "        \"Latency\": \"~300ms\",\n",
    "        \"Cost/1M\": \"$0.60\",\n",
    "        \"Status\": \"\u2705 Ready!\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\ud83d\udcca Model Comparison:\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Benefits:\")\n",
    "print(\"   \u2022 2.5x faster\")\n",
    "print(\"   \u2022 50x cheaper\")\n",
    "print(\"   \u2022 Same quality\")\n",
    "print(\"   \u2022 All tracked in MLflow automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Gradual Migration (A/B Testing)\n",
    "\n",
    "Roll out gradually with automatic tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Gradual rollout function\n",
    "@mlflow.trace\n",
    "def production_v2(text: str, rollout_pct: int = 20) -> dict:\n",
    "    \"\"\"\n",
    "    Production with gradual rollout - automatic run creation!\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        rollout_pct: % traffic to new model (0-100)\n",
    "    \"\"\"\n",
    "    use_new = random.random() * 100 < rollout_pct\n",
    "    \n",
    "    if use_new:\n",
    "        result = optimized_classifier(text)  # Auto-creates run!\n",
    "        model = \"gpt-4o-mini\"\n",
    "    else:\n",
    "        result = production_classifier(text)  # Auto-creates run!\n",
    "        model = \"gpt-4\"\n",
    "    \n",
    "    return {\"sentiment\": result, \"model\": model}\n",
    "\n",
    "print(\"\u2705 Gradual rollout ready!\")\n",
    "print(\"   Every call creates MLflow run automatically\")\n",
    "print(\"   Track cost/latency per model in real-time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 20% rollout\n",
    "print(\"\ud83d\ude80 Simulating 20% rollout...\\n\")\n",
    "\n",
    "stats = {\"gpt-4\": 0, \"gpt-4o-mini\": 0}\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = production_v2(text, rollout_pct=20)  # Auto-creates run!\n",
    "    stats[result[\"model\"]] += 1\n",
    "    print(f\"{i}. {result['model']:12s} | {result['sentiment']}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Distribution:\")\n",
    "for model, count in stats.items():\n",
    "    print(f\"   {model}: {count}/{len(test_cases)} ({count*10}%)\")\n",
    "\n",
    "print(f\"\\n\u2705 {len(test_cases)} MLflow runs created automatically!\")\n",
    "print(\"\\n\ud83d\udca1 Migration path:\")\n",
    "print(\"   5% \u2192 20% \u2192 50% \u2192 80% \u2192 100%\")\n",
    "print(\"   Monitor MLflow UI at each step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Full Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production V3: 100% new model\n",
    "@mlflow.trace\n",
    "def production_v3(text: str) -> str:\n",
    "    \"\"\"Production V3 - fully migrated - automatic run creation!\"\"\"\n",
    "    return optimized_classifier(text)\n",
    "\n",
    "# Test final version\n",
    "print(\"\ud83c\udf89 Production V3 - 100% GPT-4o-mini\\n\")\n",
    "\n",
    "samples = [\"Amazing!\", \"Terrible.\", \"It's okay.\"]\n",
    "\n",
    "for text in samples:\n",
    "    result = production_v3(text)  # Auto-creates run!\n",
    "    print(f\"'{text}' \u2192 {result}\")\n",
    "\n",
    "print(\"\\n\u2705 Migration complete!\")\n",
    "print(\"\\n\ud83d\udcc8 Achieved:\")\n",
    "print(\"   \u2022 2.5x faster\")\n",
    "print(\"   \u2022 50x cheaper\")\n",
    "print(\"   \u2022 Same quality\")\n",
    "print(\"   \u2022 Zero downtime\")\n",
    "print(\"   \u2022 All automatically tracked in MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83c\udfaf Summary: What You Achieved\n",
    "\n",
    "## Part 1: Fundamentals \u2705\n",
    "\n",
    "### 1. Automatic Tracing\n",
    "- \u2705 Every `@mlflow.trace` call creates MLflow run automatically\n",
    "- \u2705 Cost, latency, tokens tracked automatically\n",
    "- \u2705 No manual `start_run()` needed\n",
    "- \u2705 All traces visible in MLflow UI\n",
    "\n",
    "### 2. Prompt Versioning\n",
    "- \u2705 Register prompts with `register_prompt()`\n",
    "- \u2705 Retrieve any version with `get_prompt()`\n",
    "- \u2705 Compare versions in MLflow UI\n",
    "- \u2705 Roll back anytime\n",
    "\n",
    "## Part 2: Model Migration \u2705\n",
    "\n",
    "You migrated from **GPT-4 \u2192 GPT-4o-mini** with:\n",
    "\n",
    "1. **Baseline Capture** - 10 runs created automatically\n",
    "2. **New Model Test** - 10 more runs created automatically\n",
    "3. **Optimization** - Optimization runs tracked automatically\n",
    "4. **Evaluation** - 10 more runs created automatically\n",
    "5. **A/B Testing** - Traffic split tracked automatically\n",
    "6. **Full Migration** - 100% rollout with automatic tracking\n",
    "\n",
    "**Total:** ~40+ MLflow runs created **automatically** with zero manual logging!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd11 Key Takeaway\n",
    "\n",
    "### Inline API = Zero Boilerplate\n",
    "\n",
    "```python\n",
    "# \u274c OLD WAY: Manual logging\n",
    "with mlflow.start_run():\n",
    "    result = call_llm()\n",
    "    mlflow.log_metric(\"cost\", cost)\n",
    "    mlflow.log_metric(\"latency\", latency)\n",
    "    # ... more manual logging\n",
    "\n",
    "# \u2705 NEW WAY: Automatic logging\n",
    "@mlflow.trace\n",
    "def my_function():\n",
    "    return mlflow.llm_call(...)  # Everything automatic!\n",
    "```\n",
    "\n",
    "**Result:** More insights, less code, always at the frontier! \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Resources\n",
    "\n",
    "- [MLflow Auto-rewrite Prompts](https://mlflow.org/docs/latest/genai/prompt-registry/rewrite-prompts/)\n",
    "- [MLflow Evaluation Guide](https://mlflow.org/docs/latest/genai/eval-monitor/quickstart/)\n",
    "- [Prompt Management](https://mlflow.org/docs/latest/genai/prompt-registry/create-edit-prompts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}