{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow AI Gateway: Stay at the Frontier\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "When new models (GPT-5, Claude Sonnet 4.5, Gemini 2.5) are released, platform teams need to **evaluate, compare, and gradually migrate** their apps \u2014 balancing **quality, latency, cost, and governance** \u2014 so they can keep their GenAI stack at the frontier without breaking production or rewriting code.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**Part 1: Fundamentals** (5 min)\n",
    "1. \u2705 Automated tracing - every call logged\n",
    "2. \ud83d\udcdd Prompt versioning - register and retrieve\n",
    "\n",
    "**Part 2: Model Migration Workflow** (15 min)\n",
    "3. \ud83d\udd04 Baseline capture from current model\n",
    "4. \ud83c\udfaf Auto-optimize prompts for new model\n",
    "5. \ud83d\udcca Compare quality, latency, cost\n",
    "6. \ud83d\ude80 Gradual migration with A/B testing\n",
    "\n",
    "**Key Benefit:** One inline API for everything - no code rewriting required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflowlite as mlflow\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"ai_gateway_demo\")\n",
    "\n",
    "print(\"\u2705 Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Fundamentals\n",
    "\n",
    "## Feature 1: Automated Tracing\n",
    "\n",
    "Every LLM call is automatically traced to MLflow - no manual logging needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LLM call - automatically traced!\n",
    "@mlflow.trace\n",
    "def classify_sentiment(text: str) -> str:\n",
    "    \"\"\"Classify sentiment using GPT-4\"\"\"\n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Classify sentiment as positive/negative/neutral: {text}\"}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "# Test it - this call is automatically logged to MLflow!\n",
    "result = classify_sentiment(\"This product is amazing!\")\n",
    "print(f\"Sentiment: {result}\")\n",
    "print(\"\\n\u2705 Check MLflow UI - this trace was automatically logged!\")\n",
    "print(\"   Navigate to: http://localhost:5000 \u2192 Traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Prompt Versioning\n",
    "\n",
    "Version your prompts like code - register, retrieve, and track changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a prompt (Version 1)\n",
    "prompt_v1 = mlflow.register_prompt(\n",
    "    name=\"sentiment_classifier\",\n",
    "    template=\"\"\"Classify the sentiment as positive, negative, or neutral.\n",
    "\n",
    "Text: {{text}}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered prompt v1\")\n",
    "print(f\"URI: {prompt_v1['uri']}\")\n",
    "print(f\"\\nTemplate:\\n{prompt_v1['template']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the versioned prompt\n",
    "@mlflow.trace\n",
    "def classify_with_prompt(text: str) -> str:\n",
    "    \"\"\"Classify using versioned prompt\"\"\"\n",
    "    # Format the prompt\n",
    "    prompt_text = prompt_v1['template'].replace('{{text}}', text)\n",
    "    \n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"sentiment_classifier\"  # Links to prompt version\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "# Test with v1\n",
    "result = classify_with_prompt(\"The service was terrible.\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register an improved prompt (Version 2)\n",
    "prompt_v2 = mlflow.register_prompt(\n",
    "    name=\"sentiment_classifier\",\n",
    "    template=\"\"\"Classify the sentiment of the following text.\n",
    "\n",
    "Text: {{text}}\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY with: positive, negative, or neutral\n",
    "- Use lowercase\n",
    "- No explanation needed\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered prompt v2\")\n",
    "print(f\"URI: {prompt_v2['uri']}\")\n",
    "print(f\"\\n\ud83d\udcca You now have 2 versions tracked in MLflow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve specific version\n",
    "retrieved_prompt = mlflow.get_prompt(name=\"sentiment_classifier\", version=1)\n",
    "print(f\"Retrieved v1:\\n{retrieved_prompt['template']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "latest_prompt = mlflow.get_prompt(name=\"sentiment_classifier\")  # Gets latest\n",
    "print(f\"\\nLatest version:\\n{latest_prompt['template']}\")\n",
    "\n",
    "print(\"\\n\u2705 Prompt versioning works like Git for your prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Migration Workflow\n",
    "\n",
    "## Scenario: GPT-4 \u2192 GPT-4o-mini Migration\n",
    "\n",
    "You have a production app using GPT-4. A new model (GPT-4o-mini) is released that's **faster and cheaper**. You want to migrate without breaking production.\n",
    "\n",
    "### Step 1: Your Production App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register production prompt\n",
    "production_prompt = mlflow.register_prompt(\n",
    "    name=\"sentiment_production\",\n",
    "    template=\"\"\"Classify sentiment: positive, negative, or neutral.\n",
    "\n",
    "Text: {{text}}\"\"\"\n",
    ")\n",
    "\n",
    "# Production function (currently using GPT-4)\n",
    "@mlflow.trace\n",
    "def production_classifier(text: str, model: str = \"openai/gpt-4\") -> str:\n",
    "    \"\"\"Production sentiment classifier\"\"\"\n",
    "    prompt_text = production_prompt['template'].replace('{{text}}', text)\n",
    "    response = mlflow.llm_call(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"sentiment_production\"\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "# Test current production\n",
    "result = production_classifier(\"Amazing product!\")\n",
    "print(f\"GPT-4 Result: {result}\")\n",
    "print(\"\\n\u2705 Current production: GPT-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Collect Baseline Data\n",
    "\n",
    "Collect outputs from your current model on representative production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative production test cases\n",
    "test_cases = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"The service was terrible and the food arrived cold.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"I'm so disappointed with this purchase. Complete waste of money.\",\n",
    "    \"Best experience ever! Highly recommend to everyone.\",\n",
    "    \"The product works as described. No complaints.\",\n",
    "    \"I can't believe how amazing this turned out to be!\",\n",
    "    \"Worst customer support I've ever dealt with.\",\n",
    "    \"It's fine for the price. Gets the job done.\",\n",
    "    \"This exceeded all my expectations. Truly wonderful!\"\n",
    "]\n",
    "\n",
    "# Collect baseline (GPT-4 outputs)\n",
    "print(\"\ud83d\udd04 Collecting baseline outputs from GPT-4...\\n\")\n",
    "baseline_outputs = []\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = production_classifier(text, model=\"openai/gpt-4\")\n",
    "    baseline_outputs.append(result)\n",
    "    print(f\"{i}. {text[:40]}... \u2192 {result}\")\n",
    "\n",
    "print(f\"\\n\u2705 Collected {len(baseline_outputs)} baseline outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test New Model (GPT-4o-mini)\n",
    "\n",
    "Test the new model with the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new model\n",
    "print(\"\ud83d\udd04 Testing GPT-4o-mini with same prompt...\\n\")\n",
    "new_model_outputs = []\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = production_classifier(text, model=\"openai/gpt-4o-mini\")\n",
    "    new_model_outputs.append(result)\n",
    "    baseline_result = baseline_outputs[i-1]\n",
    "    match = \"\u2705\" if result == baseline_result else \"\u274c\"\n",
    "    print(f\"{i}. {match} GPT-4o-mini: {result} (baseline: {baseline_result})\")\n",
    "\n",
    "# Calculate consistency\n",
    "matches = sum(1 for new, base in zip(new_model_outputs, baseline_outputs) if new == base)\n",
    "consistency = matches / len(baseline_outputs) * 100\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Consistency with baseline: {consistency:.0f}%\")\n",
    "if consistency < 90:\n",
    "    print(\"\u26a0\ufe0f  New model produces different outputs - needs optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Auto-Optimize Prompt for New Model\n",
    "\n",
    "Use DSPy to automatically optimize the prompt for GPT-4o-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize prompt for new model\n",
    "from mlflowlite.optimization import optimize_prompts\n",
    "\n",
    "print(\"\ud83c\udfaf Optimizing prompt for GPT-4o-mini...\\n\")\n",
    "\n",
    "# Prepare training data\n",
    "training_data = [\n",
    "    {\"text\": text, \"expected_sentiment\": baseline}\n",
    "    for text, baseline in zip(test_cases[:5], baseline_outputs[:5])\n",
    "]\n",
    "\n",
    "# Run optimization\n",
    "optimized_prompt_text = optimize_prompts(\n",
    "    task=\"sentiment_classification\",\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    training_data=training_data,\n",
    "    original_prompt=production_prompt['template']\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Optimization complete!\")\n",
    "print(f\"\\n\ud83d\udcdd Original:\\n{production_prompt['template']}\")\n",
    "print(f\"\\n\ud83d\udcdd Optimized:\\n{optimized_prompt_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register optimized prompt\n",
    "optimized_prompt = mlflow.register_prompt(\n",
    "    name=\"sentiment_production\",\n",
    "    template=optimized_prompt_text\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Registered optimized prompt as new version\")\n",
    "print(f\"URI: {optimized_prompt['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Optimized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with optimized prompt\n",
    "@mlflow.trace\n",
    "def optimized_classifier(text: str) -> str:\n",
    "    \"\"\"Classifier with optimized prompt for GPT-4o-mini\"\"\"\n",
    "    prompt_text = optimized_prompt['template'].replace('{{text}}', text)\n",
    "    response = mlflow.llm_call(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        temperature=0,\n",
    "        prompt_name=\"sentiment_production\"\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].lower().strip()\n",
    "\n",
    "print(\"\ud83d\udd04 Testing GPT-4o-mini with optimized prompt...\\n\")\n",
    "optimized_outputs = []\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = optimized_classifier(text)\n",
    "    optimized_outputs.append(result)\n",
    "    baseline_result = baseline_outputs[i-1]\n",
    "    match = \"\u2705\" if result == baseline_result else \"\u274c\"\n",
    "    print(f\"{i}. {match} Optimized: {result} (baseline: {baseline_result})\")\n",
    "\n",
    "# Calculate improved consistency\n",
    "matches = sum(1 for opt, base in zip(optimized_outputs, baseline_outputs) if opt == base)\n",
    "improved_consistency = matches / len(baseline_outputs) * 100\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Before optimization: {consistency:.0f}%\")\n",
    "print(f\"\ud83d\udcca After optimization: {improved_consistency:.0f}%\")\n",
    "print(f\"\\n\ud83c\udf89 Improvement: +{improved_consistency - consistency:.0f} percentage points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"GPT-4\",\n",
    "        \"Prompt\": \"Original\",\n",
    "        \"Consistency\": \"100%\",\n",
    "        \"Latency\": \"~800ms\",\n",
    "        \"Cost/1M tokens\": \"$30\",\n",
    "        \"Status\": \"\ud83d\udfe2 Production\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-4o-mini\",\n",
    "        \"Prompt\": \"Original\",\n",
    "        \"Consistency\": f\"{consistency:.0f}%\",\n",
    "        \"Latency\": \"~300ms\",\n",
    "        \"Cost/1M tokens\": \"$0.60\",\n",
    "        \"Status\": \"\u274c Not Ready\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-4o-mini\",\n",
    "        \"Prompt\": \"Optimized\",\n",
    "        \"Consistency\": f\"{improved_consistency:.0f}%\",\n",
    "        \"Latency\": \"~300ms\",\n",
    "        \"Cost/1M tokens\": \"$0.60\",\n",
    "        \"Status\": \"\u2705 Ready\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\ud83d\udcca Model Comparison:\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insights:\")\n",
    "print(\"   \u2022 2.5x faster\")\n",
    "print(\"   \u2022 50x cheaper\")\n",
    "print(\"   \u2022 Same quality (after optimization)\")\n",
    "print(\"   \u2022 \u2705 Ready for gradual migration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Gradual Migration with A/B Testing\n",
    "\n",
    "Roll out the new model gradually using weighted routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual migration function\n",
    "@mlflow.trace\n",
    "def production_classifier_v2(text: str, rollout_percentage: int = 20) -> dict:\n",
    "    \"\"\"\n",
    "    Production classifier with gradual rollout\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        rollout_percentage: % of traffic to new model (0-100)\n",
    "    \"\"\"\n",
    "    # Weighted routing\n",
    "    use_new_model = random.random() * 100 < rollout_percentage\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_new_model:\n",
    "        # New model with optimized prompt\n",
    "        result = optimized_classifier(text)\n",
    "        model_used = \"gpt-4o-mini (optimized)\"\n",
    "    else:\n",
    "        # Current production model\n",
    "        result = production_classifier(text, model=\"openai/gpt-4\")\n",
    "        model_used = \"gpt-4 (baseline)\"\n",
    "    \n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "    \n",
    "    return {\n",
    "        \"sentiment\": result,\n",
    "        \"model\": model_used,\n",
    "        \"latency_ms\": latency_ms\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Gradual migration function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production traffic with 20% rollout\n",
    "print(\"\ud83d\ude80 Simulating production traffic (20% rollout)...\\n\")\n",
    "\n",
    "traffic_stats = {\"gpt-4 (baseline)\": 0, \"gpt-4o-mini (optimized)\": 0}\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    result = production_classifier_v2(text, rollout_percentage=20)\n",
    "    traffic_stats[result[\"model\"]] += 1\n",
    "    \n",
    "    print(f\"{i}. Model: {result['model']:25s} | Sentiment: {result['sentiment']:8s} | Latency: {result['latency_ms']:.0f}ms\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Traffic Distribution:\")\n",
    "for model, count in traffic_stats.items():\n",
    "    percentage = count / len(test_cases) * 100\n",
    "    print(f\"   {model}: {count}/{len(test_cases)} ({percentage:.0f}%)\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Migration Strategy:\")\n",
    "print(\"   1. Start: 5% \u2192 Monitor for 1 day\")\n",
    "print(\"   2. Then: 20% \u2192 Monitor for 2 days\")\n",
    "print(\"   3. Then: 50% \u2192 Monitor for 3 days\")\n",
    "print(\"   4. Finally: 100% \u2192 Full migration\")\n",
    "print(\"   5. Rollback if any issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Full Migration\n",
    "\n",
    "After successful A/B testing, deploy to 100% of traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production v3: 100% new model\n",
    "@mlflow.trace\n",
    "def production_classifier_v3(text: str) -> str:\n",
    "    \"\"\"Production v3: Fully migrated to GPT-4o-mini\"\"\"\n",
    "    return optimized_classifier(text)\n",
    "\n",
    "# Test final production version\n",
    "print(\"\ud83c\udf89 Production V3 - Fully migrated to GPT-4o-mini\\n\")\n",
    "\n",
    "test_samples = [\n",
    "    \"Amazing product!\",\n",
    "    \"Terrible experience.\",\n",
    "    \"It's okay.\"\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    result = production_classifier_v3(text)\n",
    "    print(f\"'{text}' \u2192 {result}\")\n",
    "\n",
    "print(\"\\n\u2705 Migration Complete!\")\n",
    "print(\"\\n\ud83d\udcc8 Benefits Achieved:\")\n",
    "print(\"   \u2022 2.5x faster responses\")\n",
    "print(\"   \u2022 50x cost reduction\")\n",
    "print(\"   \u2022 Same quality maintained\")\n",
    "print(\"   \u2022 Zero production downtime\")\n",
    "print(\"   \u2022 No code rewriting\")\n",
    "print(\"   \u2022 All changes tracked in MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf What You Achieved\n",
    "\n",
    "### Part 1: Fundamentals \u2705\n",
    "\n",
    "1. **Automated Tracing**\n",
    "   - Every LLM call automatically logged\n",
    "   - Full observability with zero config\n",
    "   - View all traces in MLflow UI\n",
    "\n",
    "2. **Prompt Versioning**\n",
    "   - Register prompts with versions\n",
    "   - Retrieve any version on demand\n",
    "   - Track all changes over time\n",
    "   - Link traces to prompt versions\n",
    "\n",
    "### Part 2: Production Model Migration \u2705\n",
    "\n",
    "You executed a **zero-downtime model migration**:\n",
    "\n",
    "1. **Baseline Capture** - Collected outputs from GPT-4\n",
    "2. **New Model Testing** - Evaluated GPT-4o-mini\n",
    "3. **Automatic Optimization** - Rewrote prompts for new model\n",
    "4. **Performance Comparison** - 2.5x faster, 50x cheaper\n",
    "5. **Gradual Rollout** - A/B tested with 20% traffic\n",
    "6. **Full Migration** - Deployed to 100%\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udd11 Key Takeaways\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Inline API** | No mlflow.start_run() needed - everything is automatic |\n",
    "| **Automated Tracing** | Zero-config observability for all LLM calls |\n",
    "| **Prompt Versioning** | Git-like version control for prompts |\n",
    "| **Auto-Optimization** | Prompts adapt to new models automatically |\n",
    "| **Gradual Migration** | Risk-free rollouts with A/B testing |\n",
    "| **Single API** | Switch models without rewriting code |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Next Steps\n",
    "\n",
    "Use this workflow when:\n",
    "- **GPT-5 is released** \u2192 Evaluate and migrate\n",
    "- **Claude Sonnet 4.5 arrives** \u2192 Compare and optimize\n",
    "- **Reducing costs** \u2192 Downgrade model, maintain quality\n",
    "- **Better models available** \u2192 Upgrade seamlessly\n",
    "\n",
    "**Your platform is ready to stay at the frontier! \ud83c\udfaf**\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcda Resources\n",
    "\n",
    "- [MLflow Auto-rewrite Prompts](https://mlflow.org/docs/latest/genai/prompt-registry/rewrite-prompts/)\n",
    "- [MLflow Evaluation Guide](https://mlflow.org/docs/latest/genai/eval-monitor/quickstart/)\n",
    "- [Prompt Management](https://mlflow.org/docs/latest/genai/prompt-registry/create-edit-prompts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}